#
#  Copyright 2018 ABSA Group Limited
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
#   See the License for the specific language governing permissions and
#   limitations under the License.
#

# This files contains all properties that can be set for an ingestion pipeline

# Pipeline settings
component.ingestor=spark
component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader
component.transformer.id.0=[avro.decoder]
component.transformer.class.[avro.decoder]=za.co.absa.hyperdrive.ingestor.implementation.transformer.avro.confluent.ConfluentAvroDecodingTransformer
component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetStreamWriter

# Source(Kafka) settings
reader.kafka.topic=souce-payload-topic
reader.kafka.brokers=PLAINTEXT\://broker1\:9091,SSL\://broker2:9092
#reader.option.kafka.security.protocol=SSL
#reader.option.kafka.ssl.truststore.location=path/to/your/truststore.jks
#reader.option.kafka.ssl.truststore.password=your_truststore_password
#reader.option.kafka.ssl.keystore.location=path/to/your/keystore.jks
#reader.option.kafka.ssl.keystore.password=your_keystore_password
#reader.option.kafka.ssl.key.password=your_ssl_password
#reader.option.failOnDataLoss=false

# Format(ABRiS) settings
transformer.[avro.decoder].schema.registry.url=http://localhost:8081
transformer.[avro.decoder].value.schema.id=latest
# options = {topic.name, record.name, topic.record.name}
transformer.[avro.decoder].value.schema.naming.strategy=topic.name

# for the case when the naming strategy is either record.name or topic.record.name
#transformer.[avro.decoder].value.schema.record.name = recordName
#transformer.[avro.decoder].value.schema.record.namespace = recordNamespace

# Sink(Parquet) settings
writer.common.checkpoint.location=/tmp/checkpoint-location
writer.parquet.destination.directory=/tmp/ingestion/destination-directory
#these configurations are going to be added to the Parquet DataStreamWriter
writer.parquet.options.key1=value1
writer.parquet.options.key2=value2
