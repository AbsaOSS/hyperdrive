#
#  Copyright 2019 ABSA Group Limited
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
#   See the License for the specific language governing permissions and
#   limitations under the License.
#

# This files contains all properties that can be set for an ingestion pipeline

# Pipeline settings
component.ingestor=spark
component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader
component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder
component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager
component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer
component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.AllNullableParquetStreamWriter

# Spark settings
ingestor.spark.app.name=ingestor-app-pane

# Source(Kafka) settings
reader.kafka.topic=souce-payload-topic
reader.kafka.brokers=PLAINTEXT\://broker1\:9091,SSL\://broker2:9092
#reader.option.kafka.security.protocol=SSL
#reader.option.kafka.ssl.truststore.location=path/to/your/truststore.jks
#reader.option.kafka.ssl.truststore.password=your_truststore_password
#reader.option.kafka.ssl.keystore.location=path/to/your/keystore.jks
#reader.option.kafka.ssl.keystore.password=your_keystore_password
#reader.option.kafka.ssl.key.password=your_ssl_password

# Offset management(checkpointing) settings
manager.checkpoint.base.location=/tmp/checkpoint-location

# Format(ABRiS) settings
decoder.avro.schema.registry.url=http://localhost:8081
decoder.avro.value.schema.id=latest
# options = {topic.name, record.name, topic.record.name}
decoder.avro.value.schema.naming.strategy=topic.name

# for the case when the naming strategy is either record.name or topic.record.name
#decoder.avro.value.schema.record.name = recordName
#decoder.avro.value.schema.record.namespace = recordNamespace

# options = {RETAIN_SELECTED_COLUMN_ONLY, RETAIN_ORIGINAL_SCHEMA}
decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY

# Transformations(Enceladus) settings
# comma separated list of columns to select
transformer.columns.to.select=*

# Sink(Parquet) settings
writer.parquet.destination.directory=/tmp/ingestion/destination-directory
#these configurations are going to be added to the Parquet DataStreamWriter
writer.parquet.extra.conf.1=key1=value1
writer.parquet.extra.conf.2=key2=value2
