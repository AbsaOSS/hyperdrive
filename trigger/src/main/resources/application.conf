{
  appUniqueId = "9c282190-4078-4380-8960-ce52f43b94fg"
  scheduler {
    thread.pool.size = 10
    sensors.thread.pool.size = 20
    executors.thread.pool.size = 30
    jobs.parallel.number = 4
    heart.beat = 5000
  }

  kafkaSource {
    group.id = "hyper_drive_"${appUniqueId}
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    poll.duration = 500
    max.poll.records = "3"
  }

  sparkYarnSink {
    hadoopResourceManagerUrlBase = ""
    hadoopConfDir: ""
    sparkHome: ""
    master: "yarn"
    submitTimeout: 30000,
    filesToDeploy: [
      ""
    ],
    additionalConfs: [
      {"spark.ui.port": "4004"},
       {"spark.driver.memory": "1g"},
      {"spark.executor.instances": "3"},
      {"spark.executor.cores": "3"},
      {"spark.executor.memory": "2g"}
    ]
  }

  db = {
    driver = "org.postgresql.Driver",
    url = "jdbc:postgresql://",
    user = ""
    password = ""
    keepAliveConnection = "true",
    connectionPool = "HikariCP",
    numThreads = 4
  }

}